{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nwb-dev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import pytubefix\n",
    "import os\n",
    "\n",
    "os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/opt/anaconda3/envs/ponce/lib/python3.13/site-packages/imageio_ffmpeg/binaries/ffmpeg\"\n",
    "\n",
    "#import moviepy.editor as mp\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from core.utils.func_lib import *\n",
    "from core.utils.GAN_utils import upconvGAN\n",
    "from core.utils.GAN_utils import loadBigGAN, BigGAN_wrapper\n",
    "from core.utils.GAN_invert_utils import *\n",
    "from core.utils.GAN_utils import upconvGAN\n",
    "\n",
    "from scipy.stats import special_ortho_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/watch?v=ND92YNQv0TU macaque_running 54.0 56.0\n",
      "clipped\n",
      "Moviepy - Building video /Users/dysprague/Ponce_rotation/data/videos_cropped/macaque_running.mp4.\n",
      "Moviepy - Writing video /Users/dysprague/Ponce_rotation/data/videos_cropped/macaque_running.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/dysprague/Ponce_rotation/data/videos_cropped/macaque_running.mp4\n",
      "saved\n",
      "Cropped video saved as macaque_running\n",
      "https://www.youtube.com/watch?v=mPrVMX8ACSE macaque_eating 62.0 64.0\n",
      "clipped\n",
      "Moviepy - Building video /Users/dysprague/Ponce_rotation/data/videos_cropped/macaque_eating.mp4.\n",
      "Moviepy - Writing video /Users/dysprague/Ponce_rotation/data/videos_cropped/macaque_eating.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/dysprague/Ponce_rotation/data/videos_cropped/macaque_eating.mp4\n",
      "saved\n",
      "Cropped video saved as macaque_eating\n",
      "https://www.youtube.com/watch?v=IEq5flqW8FE monkey_grooming 240.0 242.0\n",
      "clipped\n",
      "Moviepy - Building video /Users/dysprague/Ponce_rotation/data/videos_cropped/monkey_grooming.mp4.\n",
      "Moviepy - Writing video /Users/dysprague/Ponce_rotation/data/videos_cropped/monkey_grooming.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/dysprague/Ponce_rotation/data/videos_cropped/monkey_grooming.mp4\n",
      "saved\n",
      "Cropped video saved as monkey_grooming\n",
      "https://www.youtube.com/watch?v=641EOJCk8p8 monkey_fighting 143.0 145.0\n",
      "clipped\n",
      "Moviepy - Building video /Users/dysprague/Ponce_rotation/data/videos_cropped/monkey_fighting.mp4.\n",
      "Moviepy - Writing video /Users/dysprague/Ponce_rotation/data/videos_cropped/monkey_fighting.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/dysprague/Ponce_rotation/data/videos_cropped/monkey_fighting.mp4\n",
      "saved\n",
      "Cropped video saved as monkey_fighting\n",
      "https://www.youtube.com/watch?v=7utuuiw7v0U cats_jumping 11.0 13.0\n",
      "clipped\n",
      "Moviepy - Building video /Users/dysprague/Ponce_rotation/data/videos_cropped/cats_jumping.mp4.\n",
      "Moviepy - Writing video /Users/dysprague/Ponce_rotation/data/videos_cropped/cats_jumping.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/dysprague/Ponce_rotation/data/videos_cropped/cats_jumping.mp4\n",
      "saved\n",
      "Cropped video saved as cats_jumping\n",
      "https://www.youtube.com/watch?v=28FzV5OHqMU komodo 57.0 59.0\n",
      "clipped\n",
      "Moviepy - Building video /Users/dysprague/Ponce_rotation/data/videos_cropped/komodo.mp4.\n",
      "Moviepy - Writing video /Users/dysprague/Ponce_rotation/data/videos_cropped/komodo.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/dysprague/Ponce_rotation/data/videos_cropped/komodo.mp4\n",
      "saved\n",
      "Cropped video saved as komodo\n",
      "https://www.youtube.com/watch?v=p6CFBpe8zws horses 28.0 30.0\n",
      "clipped\n",
      "Moviepy - Building video /Users/dysprague/Ponce_rotation/data/videos_cropped/horses.mp4.\n",
      "Moviepy - Writing video /Users/dysprague/Ponce_rotation/data/videos_cropped/horses.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/dysprague/Ponce_rotation/data/videos_cropped/horses.mp4\n",
      "saved\n",
      "Cropped video saved as horses\n",
      "https://www.youtube.com/watch?v=xUmU_mVH_34 ambulance 40.0 42.0\n",
      "clipped\n",
      "Moviepy - Building video /Users/dysprague/Ponce_rotation/data/videos_cropped/ambulance.mp4.\n",
      "Moviepy - Writing video /Users/dysprague/Ponce_rotation/data/videos_cropped/ambulance.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/dysprague/Ponce_rotation/data/videos_cropped/ambulance.mp4\n",
      "saved\n",
      "Cropped video saved as ambulance\n",
      "https://www.youtube.com/watch?v=_D6Zi9OlUVM fan 17.0 19.0\n",
      "clipped\n",
      "Moviepy - Building video /Users/dysprague/Ponce_rotation/data/videos_cropped/fan.mp4.\n",
      "Moviepy - Writing video /Users/dysprague/Ponce_rotation/data/videos_cropped/fan.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/dysprague/Ponce_rotation/data/videos_cropped/fan.mp4\n",
      "saved\n",
      "Cropped video saved as fan\n",
      "https://www.youtube.com/watch?v=pjJBVjXhFRU soccer_ball 25.0 27.0\n",
      "clipped\n",
      "Moviepy - Building video /Users/dysprague/Ponce_rotation/data/videos_cropped/soccer_ball.mp4.\n",
      "Moviepy - Writing video /Users/dysprague/Ponce_rotation/data/videos_cropped/soccer_ball.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/dysprague/Ponce_rotation/data/videos_cropped/soccer_ball.mp4\n",
      "saved\n",
      "Cropped video saved as soccer_ball\n",
      "nan nan nan nan\n",
      "Error processing video nan: unsupported operand type(s) for +: 'float' and 'str'\n",
      "nan nan nan nan\n",
      "Error processing video nan: unsupported operand type(s) for +: 'float' and 'str'\n",
      "nan nan nan nan\n",
      "Error processing video nan: unsupported operand type(s) for +: 'float' and 'str'\n",
      "nan nan nan nan\n",
      "Error processing video nan: unsupported operand type(s) for +: 'float' and 'str'\n"
     ]
    }
   ],
   "source": [
    "def download_youtube_videos(inp_csv_file, output_videos, resolution='720p', bitrate='8M'):\n",
    "\n",
    "    vids_df = pd.read_csv(inp_csv_file)\n",
    "    links, file_names = vids_df['video'].values, vids_df['video_name'].values\n",
    "    starts, ends = vids_df['start'].values, vids_df['end'].values\n",
    "\n",
    "    for link, file_name, start, end in zip(links, file_names, starts, ends):\n",
    "        try:\n",
    "            print(link, file_name, start, end)\n",
    "            youtube = pytubefix.YouTube(link)\n",
    "            video = youtube.streams.filter(res=resolution).first()\n",
    "            video.download(output_path = output_videos)\n",
    "            filename = video.default_filename\n",
    "\n",
    "            os.rename(os.path.join(output_videos,filename), os.path.join(output_videos, file_name+'.mp4'))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {link}: {e}\")\n",
    "\n",
    "def crop_videos(inp_csv_file, input_videos, output_cropped, bitrate= '8M'):\n",
    "    vids_df = pd.read_csv(inp_csv_file)\n",
    "    links, file_names = vids_df['video'].values, vids_df['video_name'].values\n",
    "    starts, ends = vids_df['start'].values, vids_df['end'].values\n",
    "\n",
    "    for link, file_name, start, end in zip(links, file_names, starts, ends):\n",
    "        try:\n",
    "            print(link, file_name, start, end)\n",
    "            video_clip = mp.VideoFileClip(os.path.join(input_videos,file_name +'.mp4')).subclip(start, end)\n",
    "            output_path = os.path.join(output_cropped, f\"{file_name}.mp4\")\n",
    "            print('clipped')\n",
    "            video_clip.write_videofile(output_path, fps=video_clip.fps, audio=False)\n",
    "            print('saved')\n",
    "            print(f\"Cropped video saved as {file_name}\")\n",
    "            video_clip.close()\n",
    "            #os.remove(filename)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {link}: {e}\")\n",
    "\n",
    "\n",
    "inp_csv_file = \"/Users/dysprague/Ponce_rotation/data/videos.csv\"\n",
    "output_video = \"/Users/dysprague/Ponce_rotation/data/videos\"\n",
    "output_cropped = \"/Users/dysprague/Ponce_rotation/data/videos_trimmed\"\n",
    "\n",
    "#download_youtube_videos(inp_csv_file, output_video)\n",
    "\n",
    "crop_videos(inp_csv_file, output_video, output_cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate videos into component frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/dysprague/Ponce_rotation/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in os.listdir(os.path.join(data_dir, 'videos_trimmed')):\n",
    "    if not file[-4:] == '.mp4':\n",
    "        continue\n",
    "\n",
    "    frameNr = 0\n",
    "\n",
    "    capture = cv2.VideoCapture(os.path.join(data_dir, 'videos_trimmed', file))\n",
    " \n",
    "    while (True):\n",
    "    \n",
    "        success, frame = capture.read()\n",
    "\n",
    "        if not os.path.exists(os.path.join(data_dir, 'video_frames', file[:-4])):\n",
    "            os.makedirs(os.path.join(data_dir, 'video_frames', file[:-4]))\n",
    "    \n",
    "        if success:\n",
    "            cv2.imwrite(f'{data_dir}/video_frames/{file[:-4]}/frame_{frameNr}.jpg', frame)\n",
    "    \n",
    "        else:\n",
    "            break\n",
    "    \n",
    "        frameNr = frameNr+1\n",
    "    \n",
    "    capture.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert inverted frames back to videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "for folder in os.listdir(os.path.join(data_dir, 'videos_random_gen')):\n",
    "    if folder == '.DS_Store':\n",
    "        continue\n",
    "    img_folder = os.path.join(data_dir, 'videos_random_gen', folder)\n",
    "    video_name = folder + '.mp4'\n",
    "\n",
    "    frames = int(len(os.listdir(img_folder))/2)\n",
    "\n",
    "    images = [f'frame_{i}_inv.png' for i in range(frames)]\n",
    "\n",
    "    #images = [img for img in os.listdir(img_folder) if img.endswith(\"inverted.png\")]\n",
    "    frame = cv2.imread(os.path.join(img_folder, images[0]))\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(video_name, fourcc, len(images)/2, (width,height))\n",
    "\n",
    "    for image in images:\n",
    "        video.write(cv2.imread(os.path.join(img_folder, image)))\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frame_0_inverted.png', 'frame_1_inverted.png', 'frame_2_inverted.png', 'frame_3_inverted.png', 'frame_4_inverted.png', 'frame_5_inverted.png', 'frame_6_inverted.png', 'frame_7_inverted.png', 'frame_8_inverted.png', 'frame_9_inverted.png', 'frame_10_inverted.png', 'frame_11_inverted.png', 'frame_12_inverted.png', 'frame_13_inverted.png', 'frame_14_inverted.png', 'frame_15_inverted.png', 'frame_16_inverted.png', 'frame_17_inverted.png', 'frame_18_inverted.png', 'frame_19_inverted.png', 'frame_20_inverted.png', 'frame_21_inverted.png', 'frame_22_inverted.png', 'frame_23_inverted.png', 'frame_24_inverted.png', 'frame_25_inverted.png', 'frame_26_inverted.png', 'frame_27_inverted.png', 'frame_28_inverted.png', 'frame_29_inverted.png', 'frame_30_inverted.png', 'frame_31_inverted.png', 'frame_32_inverted.png', 'frame_33_inverted.png', 'frame_34_inverted.png', 'frame_35_inverted.png', 'frame_36_inverted.png', 'frame_37_inverted.png', 'frame_38_inverted.png', 'frame_39_inverted.png', 'frame_40_inverted.png', 'frame_41_inverted.png', 'frame_42_inverted.png', 'frame_43_inverted.png', 'frame_44_inverted.png', 'frame_45_inverted.png', 'frame_46_inverted.png', 'frame_47_inverted.png', 'frame_48_inverted.png', 'frame_49_inverted.png', 'frame_50_inverted.png', 'frame_51_inverted.png', 'frame_52_inverted.png', 'frame_53_inverted.png', 'frame_54_inverted.png', 'frame_55_inverted.png', 'frame_56_inverted.png', 'frame_57_inverted.png', 'frame_58_inverted.png', 'frame_59_inverted.png']\n"
     ]
    }
   ],
   "source": [
    "print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_data = {}\n",
    "total_samples = None\n",
    "\n",
    "for folder in os.listdir(os.path.join(data_dir, 'videos_inverted')):\n",
    "    if folder == '.DS_Store':\n",
    "        continue\n",
    "    \n",
    "    img_folder = os.path.join(data_dir, 'videos_inverted', folder)\n",
    "    frames = int(len(os.listdir(img_folder))/3)\n",
    "\n",
    "    for i in range(frames):\n",
    "        code = np.load(os.path.join(img_folder,f'frame_{i}_code.npy'))\n",
    "\n",
    "        if not folder in videos_data.keys():\n",
    "            videos_data[folder] = np.asarray([code])\n",
    "        else:\n",
    "            videos_data[folder] = np.vstack((videos_data[folder], code))\n",
    "\n",
    "        if total_samples is None:\n",
    "            total_samples = np.asarray([code])\n",
    "        else:\n",
    "            total_samples = np.vstack((total_samples, code))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(666, 4096)\n",
      "[(60, 4096), (48, 4096), (50, 4096), (120, 4096), (50, 4096), (120, 4096), (50, 4096), (60, 4096), (48, 4096), (60, 4096)]\n"
     ]
    }
   ],
   "source": [
    "print(total_samples.shape)\n",
    "print([value.shape for value in videos_data.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(60, 4096), (48, 4096), (50, 4096), (120, 4096), (50, 4096), (120, 4096), (50, 4096), (60, 4096), (48, 4096), (60, 4096)]\n"
     ]
    }
   ],
   "source": [
    "print([value.shape for value in videos_data.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "fig = plt.figure()\n",
    "\n",
    "for key, value in videos_data.items():\n",
    "    diff = [np.linalg.norm(value[i+1,:]-value[i,:])/value.shape[1] for i in range(value.shape[0]-1)]\n",
    "\n",
    "    plt.plot(np.linspace(0,2,len(diff)),diff, label=key)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('time (seconds)')\n",
    "plt.ylabel('Time derivative in encoded space')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "for key, value in videos_data.items():\n",
    "    theta = [np.arccos(np.dot(value[i+1,:] - value[i,:], value[i,:]-value[i-1,:])/(np.linalg.norm(value[i+1,:] - value[i,:])* np.linalg.norm(value[i,:] - value[i-1,:]))) for i in range(1,value.shape[0]-1)]\n",
    "\n",
    "    plt.plot(np.linspace(0,2,len(theta)),theta, label=key)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('time (seconds)')\n",
    "plt.ylabel('Angle between directions of subsequent frames in encoding space')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "for key, value in videos_data.items():\n",
    "    diff = [np.linalg.norm(value[i,:]-value[0,:])/value.shape[1] for i in range(value.shape[0])]\n",
    "\n",
    "    plt.plot(np.linspace(0,2,len(diff)),diff, label=key)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('time (seconds)')\n",
    "plt.ylabel('distance from original frame in encoded space')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "pca = pca.fit(total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_code = {} \n",
    "\n",
    "for key, value in videos_data.items():\n",
    "\n",
    "    reduced = pca.transform(value)\n",
    "\n",
    "    pca_code[key] = reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(60, 666), (48, 666), (50, 666), (120, 666), (50, 666), (120, 666), (50, 666), (60, 666), (48, 666), (60, 666)]\n"
     ]
    }
   ],
   "source": [
    "print([value.shape for value in pca_code.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5)\n",
    "\n",
    "code = pca_code['soccer_ball']\n",
    "\n",
    "n_points = code.shape[0]\n",
    "\n",
    "colors = cm.viridis(np.linspace(0,1,n_points))\n",
    "\n",
    "for j in range(n_points-1):\n",
    "    axs[0].scatter(code[j,0], code[j,1], color=colors[j])\n",
    "    axs[1].scatter(code[j,2], code[j,3], color=colors[j])\n",
    "    axs[2].scatter(code[j,4], code[j,5], color=colors[j])\n",
    "    axs[3].scatter(code[j,6], code[j,7], color=colors[j])\n",
    "    axs[4].scatter(code[j,8], code[j,9], color=colors[j])\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "for key, value in pca_code.items():\n",
    "\n",
    "    plt.scatter(value[:,0], value[:,1], label=key)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('PC_dimension 1')\n",
    "plt.ylabel('PC_dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for key, value in pca_code.items():\n",
    "\n",
    "    plt.scatter(value[:,2], value[:,3], label=key)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('PC_dimension 3')\n",
    "plt.ylabel('PC_dimension 4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cached device pixel ratio value was stale on window expose.  Please file a QTBUG which explains how to reproduce.\n",
      "The cached device pixel ratio value was stale on window expose.  Please file a QTBUG which explains how to reproduce.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for key, value in pca_code.items():\n",
    "\n",
    "    plt.scatter(value[:,4], value[:,5], label=key)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('PC_dimension 5')\n",
    "plt.ylabel('PC_dimension 6')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on 750 inverted image net images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/dysprague/Ponce_rotation/data'\n",
    "test_samples = None\n",
    "\n",
    "for file in os.listdir(os.path.join(data_dir, 'imagenet_test_inverted')):\n",
    "    if not file[-4:] == '.npy':\n",
    "        continue\n",
    "    \n",
    "    code  = np.load(os.path.join(data_dir, 'imagenet_test_inverted', file))\n",
    "\n",
    "    if test_samples is None:\n",
    "        test_samples = np.asarray([code])\n",
    "    else:\n",
    "        test_samples = np.vstack((test_samples, code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "pca = pca.fit(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced = pca.transform(total_samples)\n",
    "pca_code = {} \n",
    "\n",
    "for key, value in videos_data.items():\n",
    "\n",
    "    reduced = pca.transform(value)\n",
    "\n",
    "    pca_code[key] = reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(603, 4096)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = pca.explained_variance_\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(np.cumsum(eigenvalues)/np.sum(eigenvalues))\n",
    "\n",
    "#plt.xscale('log')\n",
    "\n",
    "plt.xlabel('#PC dimensions used')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt \n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for key, value in pca_code.items():\n",
    "\n",
    "    plt.scatter(value[:,0], value[:,1], label=key)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('PC_dimension 1')\n",
    "plt.ylabel('PC_dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9t/68zflby9385dfmd93cm_b7ph0000gn/T/ipykernel_44666/3007476109.py:9: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt \n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "full_pca = pca.transform(test_samples)\n",
    "\n",
    "plt.scatter(full_pca[:,0], full_pca[:,1])\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('PC_dimension 1')\n",
    "plt.ylabel('PC_dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 603)\n"
     ]
    }
   ],
   "source": [
    "print(pca_code['macaque_eating'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "\n",
    "x_train = None \n",
    "y_train = None\n",
    "\n",
    "for key, value in pca_code.items():\n",
    "    if x_train is None:\n",
    "        x_train = value \n",
    "        y_train = np.expand_dims(np.asarray([key for i in range(value.shape[0])]), axis=1)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        x_train = np.vstack((x_train, value))\n",
    "        y_train = np.vstack((y_train, np.expand_dims(np.asarray([key for i in range(value.shape[0])]), axis=1)))\n",
    "\n",
    "y_train = np.squeeze(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(666,)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = np.zeros(x_train.shape[1]-1)\n",
    "\n",
    "for i in range(1,len(accs)):\n",
    "    svm_model_linear = SVC(kernel = 'linear', C = 1).fit(x_train[:,:i], y_train) \n",
    "    \n",
    "    # model accuracy for X_test   \n",
    "    accs[i] = svm_model_linear.score(x_train[:,:i], y_train) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = np.zeros(x_train.shape[1]-1)\n",
    "\n",
    "for i in range(1,len(accs)):\n",
    "    svm_model_linear = SVC(kernel = 'linear', C = 1).fit(x_train[:300,:i], y_train[:300]) \n",
    "    \n",
    "    # model accuracy for X_test   \n",
    "    accs[i] = svm_model_linear.score(x_train[:300,:i], y_train[:300]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure() \n",
    "\n",
    "plt.plot(accs)\n",
    "plt.xlabel('#PC dimensions used')\n",
    "plt.ylabel('Accuracy of linear classification')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m code \u001b[38;5;241m=\u001b[39m pca_code[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mambulance\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m n_points \u001b[38;5;241m=\u001b[39m code\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m colors \u001b[38;5;241m=\u001b[39m cm\u001b[38;5;241m.\u001b[39mviridis(np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,n_points))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_points\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     10\u001b[0m     axs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mscatter(code[j,\u001b[38;5;241m0\u001b[39m], code[j,\u001b[38;5;241m1\u001b[39m], color\u001b[38;5;241m=\u001b[39mcolors[j])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cm' is not defined"
     ]
    }
   ],
   "source": [
    "fig, axs = plt.subplots(5)\n",
    "\n",
    "code = pca_code['ambulance']\n",
    "\n",
    "n_points = code.shape[0]\n",
    "\n",
    "colors = cm.viridis(np.linspace(0,1,n_points))\n",
    "\n",
    "for j in range(n_points-1):\n",
    "    axs[0].scatter(code[j,0], code[j,1], color=colors[j])\n",
    "    axs[1].scatter(code[j,2], code[j,3], color=colors[j])\n",
    "    axs[2].scatter(code[j,4], code[j,5], color=colors[j])\n",
    "    axs[3].scatter(code[j,6], code[j,7], color=colors[j])\n",
    "    axs[4].scatter(code[j,8], code[j,9], color=colors[j])\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import special_ortho_group\n",
    "num_dim=60\n",
    "x = special_ortho_group.rvs(num_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturb image invert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "invert_path = '/Users/dysprague/Ponce_rotation/data/videos_inverted'\n",
    "\n",
    "codes = {}\n",
    "\n",
    "for folder in os.listdir(invert_path):\n",
    "    if folder == '.DS_Store':\n",
    "        continue\n",
    "\n",
    "    img_folder = os.path.join(invert_path, folder)\n",
    "    frames = int(len(os.listdir(img_folder))/3)\n",
    "\n",
    "    for i in range(frames):\n",
    "        code = np.load(os.path.join(img_folder,f'frame_{i}_code.npy')) \n",
    "\n",
    "        if not folder in codes.keys():\n",
    "            codes[folder] = np.asarray([code])\n",
    "        else:\n",
    "            codes[folder] = np.vstack((codes[folder], code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate = special_ortho_group.rvs(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate_code = {}\n",
    "noise_code = {} \n",
    "reverse_code = {} \n",
    "check_code = {} \n",
    "\n",
    "for key, value in codes.items():\n",
    "    \n",
    "    centered_code = value - np.ones((value.shape[0],1)) @ value[0].reshape(-1,1).transpose()\n",
    "\n",
    "    r_use = rotate\n",
    "\n",
    "    np.random.shuffle(r_use) \n",
    "\n",
    "    rotation_matrix = r_use\n",
    "\n",
    "    rotated_code = (rotation_matrix @ centered_code.transpose()).transpose()\n",
    "\n",
    "    uncentered_rotated = rotated_code + np.ones((value.shape[0],1)) @ value[0].reshape(-1,1).transpose()\n",
    "\n",
    "    rotate_code[key] = uncentered_rotated \n",
    "\n",
    "    check_code[key] = value[0,:].reshape(-1,1).transpose()\n",
    "    reverse_code[key] = value[0,:].reshape(-1,1).transpose()\n",
    "    noise_code[key] = value[0,:].reshape(-1,1).transpose()\n",
    "\n",
    "    for i in range(1,value.shape[0]):\n",
    "\n",
    "        diff = np.squeeze(value[i,:]) - np.squeeze(value[i-1,:])\n",
    "\n",
    "        control = np.squeeze(value[i-1,:]) + diff \n",
    "\n",
    "        reverse = np.squeeze(reverse_code[key][i-1,:]) - diff \n",
    "\n",
    "        noise = np.random.normal(0, 0.2, diff.shape)\n",
    "\n",
    "        noisy_update = np.squeeze(noise_code[key][i-1,:]) + noise\n",
    "\n",
    "        check_code[key] = np.vstack((check_code[key], control.reshape(-1,1).transpose()))\n",
    "        reverse_code[key] = np.vstack((reverse_code[key], reverse.reshape(-1,1).transpose()))\n",
    "        noise_code[key] = np.vstack((noise_code[key], noisy_update.reshape(-1,1).transpose()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dysprague/Ponce_rotation/core/utils/GAN_utils.py:179: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  SD = torch.load(savepath[name])\n"
     ]
    }
   ],
   "source": [
    "G = upconvGAN('fc6').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/dysprague/Ponce_rotation/data/images_perturbed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1023.5905\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in codes.keys():\n",
    "    check = check_code[key]\n",
    "    reverse = reverse_code[key]\n",
    "    noise = noise_code[key]\n",
    "    rot = rotate_code[key]\n",
    "\n",
    "    for i in range(check.shape[0]):\n",
    "\n",
    "        os.makedirs(os.path.join(save_dir, 'control', key), exist_ok=True)\n",
    "        os.makedirs(os.path.join(save_dir, 'noise', key), exist_ok=True)\n",
    "        os.makedirs(os.path.join(save_dir, 'reverse', key), exist_ok=True)\n",
    "        os.makedirs(os.path.join(save_dir, 'rotation', key), exist_ok=True)\n",
    "\n",
    "        noise_im = torch.tensor(noise[i,:]).float()\n",
    "        img_noise = G.visualize(noise_im.detach())\n",
    "        img_noise_save = ToPILImage()(np.squeeze(img_noise))\n",
    "        img_noise_save.save(os.path.join(save_dir, 'noise', key, f\"frame_{i}_inverted.png\"))\n",
    "        np.save(os.path.join(save_dir, 'noise', key, f\"frame_{i}_code.npy\"), arr= noise_im )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in codes.keys():\n",
    "    check = check_code[key]\n",
    "    reverse = reverse_code[key]\n",
    "    noise = noise_code[key]\n",
    "    rot = rotate_code[key]\n",
    "\n",
    "    for i in range(check.shape[0]):\n",
    "\n",
    "        os.makedirs(os.path.join(save_dir, 'control', key), exist_ok=True)\n",
    "        os.makedirs(os.path.join(save_dir, 'noise', key), exist_ok=True)\n",
    "        os.makedirs(os.path.join(save_dir, 'reverse', key), exist_ok=True)\n",
    "        os.makedirs(os.path.join(save_dir, 'rotation', key), exist_ok=True)\n",
    "\n",
    "        check_im = torch.tensor(check[i,:]).float()\n",
    "        img_check = G.visualize(check_im.detach())\n",
    "        img_check_save = ToPILImage()(np.squeeze(img_check))\n",
    "        img_check_save.save(os.path.join(save_dir, 'control', key, f\"frame_{i}_inverted.png\"))\n",
    "        np.save(os.path.join(save_dir, 'control', key, f\"frame_{i}_code.npy\"), arr=check_im)\n",
    "\n",
    "        reverse_im = torch.tensor(reverse[i,:]).float()\n",
    "        img_reverse = G.visualize(reverse_im.detach())\n",
    "        img_reverse_save = ToPILImage()(np.squeeze(img_reverse))\n",
    "        img_reverse_save.save(os.path.join(save_dir, 'reverse', key, f\"frame_{i}_inverted.png\"))\n",
    "        np.save(os.path.join(save_dir, 'reverse', key, f\"frame_{i}_code.npy\"), arr= reverse_im )\n",
    "\n",
    "        noise_im = torch.tensor(noise[i,:]).float()\n",
    "        img_noise = G.visualize(noise_im.detach())\n",
    "        img_noise_save = ToPILImage()(np.squeeze(img_noise))\n",
    "        img_noise_save.save(os.path.join(save_dir, 'noise', key, f\"frame_{i}_inverted.png\"))\n",
    "        np.save(os.path.join(save_dir, 'noise', key, f\"frame_{i}_code.npy\"), arr= noise_im )\n",
    "\n",
    "        rot_im = torch.tensor(rot[i,:]).float()\n",
    "        img_rot = G.visualize(rot_im.detach())\n",
    "        img_rot_save = ToPILImage()(np.squeeze(img_rot))\n",
    "        img_rot_save.save(os.path.join(save_dir, 'rotation', key, f\"frame_{i}_inverted.png\"))\n",
    "        np.save(os.path.join(save_dir, 'rotation', key, f\"frame_{i}_code.npy\"), arr= rot_im )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "for folder in os.listdir(os.path.join(save_dir)):\n",
    "    if folder == '.DS_Store':\n",
    "        continue\n",
    "\n",
    "    for img_folder in os.listdir(os.path.join(save_dir,folder)):\n",
    "        if img_folder =='.DS_Store':\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(save_dir, folder, img_folder)\n",
    "        video_name = os.path.join(save_dir, '..', 'videos_perturbed', folder, img_folder) + '.mp4'\n",
    "\n",
    "        frames = int(len(os.listdir(img_path))/2)\n",
    "\n",
    "        images = [f'frame_{i}_inverted.png' for i in range(frames)]\n",
    "\n",
    "        #images = [img for img in os.listdir(img_folder) if img.endswith(\"inverted.png\")]\n",
    "        frame = cv2.imread(os.path.join(img_path, images[0]))\n",
    "        height, width, layers = frame.shape\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        video = cv2.VideoWriter(video_name, fourcc, len(images)/2, (width,height))\n",
    "\n",
    "        for image in images:\n",
    "            video.write(cv2.imread(os.path.join(img_path, image)))\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different titrations of reversal and rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import expm, logm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_scaling_factor = 0.5\n",
    "scaling_factor = 0.1\n",
    "noise_factor = 0.5\n",
    "\n",
    "r_use = rotate \n",
    "\n",
    "rotation_matrix = r_use\n",
    "\n",
    "log_rotation_matrix = logm(rotation_matrix)\n",
    "\n",
    "scaled_log_rotation_matrix = rotation_scaling_factor * log_rotation_matrix\n",
    "\n",
    "scaled_rotation_matrix = expm(scaled_log_rotation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate_code = {}\n",
    "reverse_code = {} \n",
    "check_code = {} \n",
    "noise_code = {}\n",
    "\n",
    "for key, value in codes.items():\n",
    "    \n",
    "    centered_code = value - np.ones((value.shape[0],1)) @ value[0].reshape(-1,1).transpose()\n",
    "\n",
    "    rotated_code = (scaled_rotation_matrix @ centered_code.transpose()).transpose()\n",
    "\n",
    "    uncentered_rotated = rotated_code + np.ones((value.shape[0],1)) @ value[0].reshape(-1,1).transpose()\n",
    "\n",
    "    rotate_code[key] = uncentered_rotated \n",
    "\n",
    "    check_code[key] = value[0,:].reshape(-1,1).transpose()\n",
    "    reverse_code[key] = value[0,:].reshape(-1,1).transpose()\n",
    "    noise_code[key] = value[0,:].reshape(-1,1).transpose()\n",
    "\n",
    "    for i in range(1,value.shape[0]):\n",
    "\n",
    "        diff = np.squeeze(value[i,:]) - np.squeeze(value[i-1,:])\n",
    "\n",
    "        control = np.squeeze(check_code[key][i-1,:]) + diff*scaling_factor\n",
    "\n",
    "        reverse = np.squeeze(reverse_code[key][i-1,:]) - diff*scaling_factor \n",
    "\n",
    "        noise = np.random.normal(0, noise_factor, diff.shape)\n",
    "\n",
    "        noisy_update = np.squeeze(noise_code[key][i-1,:]) + noise\n",
    "\n",
    "        check_code[key] = np.vstack((check_code[key], control.reshape(-1,1).transpose()))\n",
    "        reverse_code[key] = np.vstack((reverse_code[key], reverse.reshape(-1,1).transpose()))\n",
    "        noise_code[key] = np.vstack((noise_code[key], noisy_update.reshape(-1,1).transpose()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/dysprague/Ponce_rotation/data/images_perturbed_test'\n",
    "\n",
    "for key in codes.keys():\n",
    "    check = check_code[key]\n",
    "    reverse = reverse_code[key]\n",
    "    rot = rotate_code[key]\n",
    "    noise = noise_code[key]\n",
    "\n",
    "    for i in range(check.shape[0]):\n",
    "\n",
    "        os.makedirs(os.path.join(save_dir, f'control_{str(scaling_factor)}', key), exist_ok=True)\n",
    "        os.makedirs(os.path.join(save_dir, f'reverse_{str(scaling_factor)}', key), exist_ok=True)\n",
    "        os.makedirs(os.path.join(save_dir, f'rotation_{str(rotation_scaling_factor)}', key), exist_ok=True)\n",
    "        os.makedirs(os.path.join(save_dir, f'noise_{str(noise_factor)}', key), exist_ok=True)\n",
    "\n",
    "        check_im = torch.tensor(check[i,:]).float()\n",
    "        img_check = G.visualize(check_im.detach())\n",
    "        img_check_save = ToPILImage()(np.squeeze(img_check))\n",
    "        img_check_save.save(os.path.join(save_dir, f'control_{str(scaling_factor)}', key, f\"frame_{i}_inverted.png\"))\n",
    "        np.save(os.path.join(save_dir, f'control_{str(scaling_factor)}', key, f\"frame_{i}_code.npy\"), arr=check_im)\n",
    "\n",
    "        reverse_im = torch.tensor(reverse[i,:]).float()\n",
    "        img_reverse = G.visualize(reverse_im.detach())\n",
    "        img_reverse_save = ToPILImage()(np.squeeze(img_reverse))\n",
    "        img_reverse_save.save(os.path.join(save_dir, f'reverse_{str(scaling_factor)}', key, f\"frame_{i}_inverted.png\"))\n",
    "        np.save(os.path.join(save_dir, f'reverse_{str(scaling_factor)}', key, f\"frame_{i}_code.npy\"), arr= reverse_im )\n",
    "\n",
    "        noise_im = torch.tensor(noise[i,:]).float()\n",
    "        img_noise = G.visualize(noise_im.detach())\n",
    "        img_noise_save = ToPILImage()(np.squeeze(img_noise))\n",
    "        img_noise_save.save(os.path.join(save_dir, f'noise_{str(noise_factor)}', key, f\"frame_{i}_inverted.png\"))\n",
    "        np.save(os.path.join(save_dir, f'noise_{str(noise_factor)}', key, f\"frame_{i}_code.npy\"), arr= noise_im )\n",
    "\n",
    "        rot_im = torch.tensor(rot[i,:]).float()\n",
    "        img_rot = G.visualize(rot_im.detach())\n",
    "        img_rot_save = ToPILImage()(np.squeeze(img_rot))\n",
    "        img_rot_save.save(os.path.join(save_dir, f'rotation_{str(rotation_scaling_factor)}', key, f\"frame_{i}_inverted.png\"))\n",
    "        np.save(os.path.join(save_dir, f'rotation_{str(rotation_scaling_factor)}', key, f\"frame_{i}_code.npy\"), arr= rot_im )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir(os.path.join(save_dir)):\n",
    "    if folder == '.DS_Store':\n",
    "        continue\n",
    "\n",
    "    for img_folder in os.listdir(os.path.join(save_dir,folder)):\n",
    "        if img_folder =='.DS_Store':\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(save_dir, folder, img_folder)\n",
    "        os.makedirs(os.path.join(save_dir, '..', 'videos_perturbed_test', folder), exist_ok=True)\n",
    "        video_name = os.path.join(save_dir, '..', 'videos_perturbed_test', folder, img_folder) + '.mp4'\n",
    "\n",
    "        frames = int(len(os.listdir(img_path))/2)\n",
    "\n",
    "        images = [f'frame_{i}_inverted.png' for i in range(frames)]\n",
    "\n",
    "        #images = [img for img in os.listdir(img_folder) if img.endswith(\"inverted.png\")]\n",
    "        frame = cv2.imread(os.path.join(img_path, images[0]))\n",
    "        height, width, layers = frame.shape\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        video = cv2.VideoWriter(video_name, fourcc, len(images)/2, (width,height))\n",
    "\n",
    "        for image in images:\n",
    "            video.write(cv2.imread(os.path.join(img_path, image)))\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print([i*0.1 for i in range(1,11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_code = {} \n",
    "\n",
    "for key, value in codes.items():\n",
    "\n",
    "    interpolation_code[key] = value[0,:].reshape(-1,1).transpose()\n",
    "\n",
    "    for i in range(1,value.shape[0]):\n",
    "\n",
    "        diff = np.squeeze(value[i,:]) - np.squeeze(value[i-1,:])\n",
    "\n",
    "        interp_start = np.squeeze(interpolation_code[key][-1,:])\n",
    "\n",
    "        for j in range(1,11):\n",
    "\n",
    "            update = interp_start + diff*j*0.1\n",
    "\n",
    "            interpolation_code[key] = np.vstack((interpolation_code[key], update.reshape(-1,1).transpose()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/dysprague/Ponce_rotation/data/images_perturbed_test'\n",
    "\n",
    "for key in codes.keys():\n",
    "    interp = interpolation_code[key]\n",
    "\n",
    "    for i in range(interp.shape[0]):\n",
    "\n",
    "        os.makedirs(os.path.join(save_dir, f'interp_{str(0.1)}', key), exist_ok=True)\n",
    "\n",
    "        interp_im = torch.tensor(interp[i,:]).float()\n",
    "        img_interp = G.visualize(interp_im.detach())\n",
    "        img_interp_save = ToPILImage()(np.squeeze(img_interp))\n",
    "        img_interp_save.save(os.path.join(save_dir, f'interp_{str(0.1)}', key, f\"frame_{i}_inverted.png\"))\n",
    "        np.save(os.path.join(save_dir, f'interp_{str(0.1)}', key, f\"frame_{i}_code.npy\"), arr=interp_im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir(os.path.join(save_dir)):\n",
    "    if folder == '.DS_Store':\n",
    "        continue\n",
    "\n",
    "    for img_folder in os.listdir(os.path.join(save_dir,folder)):\n",
    "        if img_folder =='.DS_Store':\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(save_dir, folder, img_folder)\n",
    "        os.makedirs(os.path.join(save_dir, '..', 'videos_perturbed_test', folder), exist_ok=True)\n",
    "        video_name = os.path.join(save_dir, '..', 'videos_perturbed_test', folder, img_folder) + '.mp4'\n",
    "\n",
    "        frames = int(len(os.listdir(img_path))/2)\n",
    "\n",
    "        images = [f'frame_{i}_inverted.png' for i in range(frames)]\n",
    "\n",
    "        #images = [img for img in os.listdir(img_folder) if img.endswith(\"inverted.png\")]\n",
    "        frame = cv2.imread(os.path.join(img_path, images[0]))\n",
    "        height, width, layers = frame.shape\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "        if folder == 'interp_0.1':\n",
    "            video = cv2.VideoWriter(video_name, fourcc, len(images)/10, (width,height))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for image in images:\n",
    "            video.write(cv2.imread(os.path.join(img_path, image)))\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(frame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify image similarity across subsequent frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nwb-dev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import pytubefix\n",
    "import os\n",
    "\n",
    "#os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/opt/anaconda3/envs/ponce/lib/python3.13/site-packages/imageio_ffmpeg/binaries/ffmpeg\"\n",
    "\n",
    "#import moviepy.editor as mp\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from core.utils.func_lib import *\n",
    "from core.utils.GAN_utils import upconvGAN\n",
    "from core.utils.GAN_utils import loadBigGAN, BigGAN_wrapper\n",
    "from core.utils.GAN_invert_utils import *\n",
    "from core.utils.GAN_utils import upconvGAN\n",
    "\n",
    "from scipy.stats import special_ortho_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils.image_similarity import *\n",
    "from core.utils.CNN_scorers import TorchScorer\n",
    "\n",
    "import cv2 \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = '/Users/dysprague/Ponce_rotation/data'\n",
    "data_dir = r\"/n/home09/dsprague/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nwb-dev/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/nwb-dev/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'0': OrderedDict({'inshape': (3, 227, 227), 'outshape': (3, 227, 227)}), '1': OrderedDict({'inshape': (3, 227, 227), 'outshape': (64, 56, 56)}), '2': OrderedDict({'inshape': (64, 56, 56), 'outshape': (64, 56, 56)}), '3': OrderedDict({'inshape': (64, 56, 56), 'outshape': (64, 27, 27)}), '4': OrderedDict({'inshape': (64, 27, 27), 'outshape': (192, 27, 27)}), '5': OrderedDict({'inshape': (192, 27, 27), 'outshape': (192, 27, 27)}), '6': OrderedDict({'inshape': (192, 27, 27), 'outshape': (192, 13, 13)}), '7': OrderedDict({'inshape': (192, 13, 13), 'outshape': (384, 13, 13)}), '8': OrderedDict({'inshape': (384, 13, 13), 'outshape': (384, 13, 13)}), '9': OrderedDict({'inshape': (384, 13, 13), 'outshape': (256, 13, 13)}), '10': OrderedDict({'inshape': (256, 13, 13), 'outshape': (256, 13, 13)}), '11': OrderedDict({'inshape': (256, 13, 13), 'outshape': (256, 13, 13)}), '12': OrderedDict({'inshape': (256, 13, 13), 'outshape': (256, 13, 13)}), '13': OrderedDict({'inshape': (256, 13, 13), 'outshape': (256, 6, 6)}), '14': OrderedDict({'inshape': (3, 227, 227), 'outshape': (256, 6, 6)}), '15': OrderedDict({'inshape': (256, 6, 6), 'outshape': (256, 6, 6)}), '16': OrderedDict({'inshape': (9216,), 'outshape': (9216,)}), '17': OrderedDict({'inshape': (9216,), 'outshape': (4096,)}), '18': OrderedDict({'inshape': (4096,), 'outshape': (4096,)}), '19': OrderedDict({'inshape': (4096,), 'outshape': (4096,)}), '20': OrderedDict({'inshape': (4096,), 'outshape': (4096,)}), '21': OrderedDict({'inshape': (4096,), 'outshape': (4096,)}), '22': OrderedDict({'inshape': (4096,), 'outshape': (1000,)}), '23': OrderedDict({'inshape': (9216,), 'outshape': (1000,)}), '24': OrderedDict({'inshape': (3, 227, 227), 'outshape': (1000,)})})\n",
      "{'Image': '0', '.features.Conv2d0': '1', '.features.ReLU1': '2', '.features.MaxPool2d2': '3', '.features.Conv2d3': '4', '.features.ReLU4': '5', '.features.MaxPool2d5': '6', '.features.Conv2d6': '7', '.features.ReLU7': '8', '.features.Conv2d8': '9', '.features.ReLU9': '10', '.features.Conv2d10': '11', '.features.ReLU11': '12', '.features.MaxPool2d12': '13', '.features': '14', '.AdaptiveAvgPool2davgpool': '15', '.classifier.Dropout0': '16', '.classifier.Linear1': '17', '.classifier.ReLU2': '18', '.classifier.Dropout3': '19', '.classifier.Linear4': '20', '.classifier.ReLU5': '21', '.classifier.Linear6': '22', '.classifier': '23', '.AlexNet': '24'}\n",
      "['.features.Conv2d3', '.features.Conv2d6', '.features.Conv2d8', '.features.Conv2d10']\n",
      "False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m sim_scorer\u001b[38;5;241m.\u001b[39mfirst_image_batch \u001b[38;5;241m=\u001b[39m batch_1 \n\u001b[1;32m     20\u001b[0m sim_scorer\u001b[38;5;241m.\u001b[39msecond_image_batch \u001b[38;5;241m=\u001b[39m batch_2\n\u001b[0;32m---> 22\u001b[0m mean_simmat, simmat \u001b[38;5;241m=\u001b[39m sim_scorer\u001b[38;5;241m.\u001b[39mget_CCN_distance()\n",
      "File \u001b[0;32m~/Ponce_rotation/core/utils/image_similarity.py:93\u001b[0m, in \u001b[0;36mTorchImageDistance.get_CCN_distance\u001b[0;34m(self, similarity, units_slice)\u001b[0m\n\u001b[1;32m     91\u001b[0m encoded_second_image_batch_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scorer, layer_list \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorch_scorer_list, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers_list):\n\u001b[0;32m---> 93\u001b[0m     encoded_first_image_batch, _ \u001b[38;5;241m=\u001b[39m encode_image(scorer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst_image_batch, key\u001b[38;5;241m=\u001b[39mlayer_list,\n\u001b[1;32m     94\u001b[0m                                     RFresize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, cat_layes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     95\u001b[0m     encoded_second_image_batch, _ \u001b[38;5;241m=\u001b[39m encode_image(scorer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msecond_image_batch, key\u001b[38;5;241m=\u001b[39mlayer_list,\n\u001b[1;32m     96\u001b[0m                                     RFresize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, cat_layes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     97\u001b[0m     encoded_first_image_batch_list\u001b[38;5;241m.\u001b[39mappend(encoded_first_image_batch)\n",
      "File \u001b[0;32m~/Ponce_rotation/core/utils/func_lib.py:589\u001b[0m, in \u001b[0;36mencode_image\u001b[0;34m(scorer, imgtsr, key, RFresize, corner, imgsize, cat_layes)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"return a 2d array / tensor of activations for a image tensor\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;124;03mimgtsr: (Nimgs, C, H, W)\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;124;03mactmat: (Npop, Nimages) torch tensor\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;124;03m    if key is in the dict, then return a single actmat of shape (imageN, unitN)\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;66;03m#TODO: make this work for larger image dataset\u001b[39;00m\n\u001b[0;32m--> 589\u001b[0m _, recordings \u001b[38;5;241m=\u001b[39m scorer\u001b[38;5;241m.\u001b[39mscore_tsr(imgtsr)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RFresize: imgtsr \u001b[38;5;241m=\u001b[39m resize_and_pad_tsr(imgtsr, imgsize, corner, )\n\u001b[1;32m    591\u001b[0m _, recordings \u001b[38;5;241m=\u001b[39m scorer\u001b[38;5;241m.\u001b[39mscore_tsr(imgtsr)\n",
      "File \u001b[0;32m~/Ponce_rotation/core/utils/CNN_scorers.py:444\u001b[0m, in \u001b[0;36mTorchScorer.score_tsr\u001b[0;34m(self, img_tsr, with_grad, B, input_scale)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m csr \u001b[38;5;241m<\u001b[39m imgn:\n\u001b[1;32m    443\u001b[0m     csr_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(csr \u001b[38;5;241m+\u001b[39m B, imgn)\n\u001b[0;32m--> 444\u001b[0m     img_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(img_tsr[csr:csr_end,:,:,:], input_scale\u001b[38;5;241m=\u001b[39minput_scale)\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/Ponce_rotation/core/utils/CNN_scorers.py:378\u001b[0m, in \u001b[0;36mTorchScorer.preprocess\u001b[0;34m(self, img, input_scale)\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resz_out_tsr\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(img) \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 378\u001b[0m     img_tsr \u001b[38;5;241m=\u001b[39m (img\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;241m/\u001b[39m input_scale \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mRGBmean) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mRGBstd\n\u001b[1;32m    379\u001b[0m     resz_out_tsr \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(img_tsr, imgsize, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    380\u001b[0m                                  align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resz_out_tsr\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nwb-dev/lib/python3.12/site-packages/torch/cuda/__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "## Verify close image similarity for original vs inverted images in each video\n",
    "\n",
    "#for folder in os.listdir(os.path.join(data_dir, 'videos_inverted')):\n",
    "\n",
    "img_path  = os.path.join(data_dir, 'videos_inverted', 'horses')\n",
    "\n",
    "sim_scorer = TorchImageDistance()\n",
    "#sim_scorer.torch_scorer_list = [TorchScorer('resnet50'), TorchScorer('alexnet')]\n",
    "#sim_scorer.layers_list = [['.layer1.2.BatchNorm2dbn3', '.layer2.3.BatchNorm2dbn3', '.layer3.5.BatchNorm2dbn3', '.layer4.2.BatchNorm2dbn3'],['.features.Conv2d3', '.features.Conv2d6', '.features.Conv2d8', '.features.Conv2d10']]\n",
    "\n",
    "frames = int(len(os.listdir(img_path))/3)\n",
    "\n",
    "images_1 = [f'frame_{i}_original.png' for i in range(frames)]\n",
    "images_2 = [f'frame_{i}_original.png' for i in range(frames)]\n",
    "\n",
    "batch_1  = torch.from_numpy(np.asarray([cv2.imread(os.path.join(img_path, image)).transpose(2,0,1) for image in images_1]))\n",
    "batch_2 = torch.from_numpy(np.asarray([cv2.imread(os.path.join(img_path, image)).transpose(2,0,1) for image in images_2]))\n",
    "\n",
    "sim_scorer.first_image_batch = batch_1 \n",
    "sim_scorer.second_image_batch = batch_2\n",
    "\n",
    "mean_simmat, simmat = sim_scorer.get_CCN_distance()\n",
    "\n",
    "#sim_scorer.first_image_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ponce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
