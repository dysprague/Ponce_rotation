{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(r\"C:\\Users\\Alireza\\Documents\\Git\\Cosine-Project\")\n",
    "from inSilico_experiments.utils.pothook_analysis_lib import *\n",
    "from torchvision.transforms import ToTensor, ToPILImage, Pad\n",
    "from torchvision.utils import make_grid\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata \n",
    "data_path = r\"N:\\PonceLab\\Users\\Alireza\\insilico_experiments\\Alexnet_remonstration_across_different_layer_062023\\post_processed\"\n",
    "\n",
    "metadata_df_cosine = pd.read_hdf(os.path.join(data_path, \"metadata_df_with_sim_index.h5\"), key=\"metadata_df\")\n",
    "\n",
    "save_root = r\"N:\\PonceLab\\Users\\Alireza\\insilico_experiments\\Alexnet_remonstration_across_different_layer_062023\\results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('1687393229_3244256', '1687393249_550689', '1687393268_530598', '1687393287_6103828', '1687393306_5532094', '1687393326_8358296', '1687393346_1224729', '1687393365_2815180', '1687393387_9144676', '1687393406_5828132')\n"
     ]
    }
   ],
   "source": [
    "from inSilico_experiments.utils.CosineDataset import SimpleDataset\n",
    "from torch.utils.data import DataLoader\n",
    "layer_short_list= metadata_df_cosine[\"layer_short\"].unique()\n",
    "# remove the NaN values from the layer_short_list\n",
    "similarity_metric = \"cosine\"\n",
    "pop_size = 32\n",
    "gen_rerun_id = 0\n",
    "output_type = \"best_gen_imgs_RF_masked\"\n",
    "RF_treshold = 2\n",
    "pop_resampling_id = 0\n",
    "smpling_type = \"random\"\n",
    "layer_short = \"conv5\"\n",
    "sub_meta_df = metadata_df_cosine[\\\n",
    "                (metadata_df_cosine[\"output_type\"] == \"best_gen_imgs_RF_masked\") & (\n",
    "                metadata_df_cosine[\"layer_short\"]==layer_short) & (\n",
    "                metadata_df_cosine[\"sub_pop_type\"] == smpling_type) & (\n",
    "                metadata_df_cosine[\"similarity_metric\"] == similarity_metric) ]\n",
    "path_list = [os.path.join(sub_meta_df[\"data_root\"][i], f\"{sub_meta_df.index[i]}.jpg\") for i in range(len(sub_meta_df))]\n",
    "label_list = sub_meta_df.index.values\n",
    "\n",
    "transform = Compose([\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "dataset = SimpleDataset(path_list, label_list, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=10, num_workers=5)\n",
    "\n",
    "for i, data in enumerate(dataloader):\n",
    "    print(i, data[1])\n",
    "    break\n",
    "for images_batch, labels_batch in dataloader:\n",
    "    break\n",
    "images_batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 227, 227])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths1 = np.array((path_list[1:3], path_list[4:6]))\n",
    "labels1 = np.array((label_list[1:3], label_list[4:6]))  \n",
    "# empty numpy array\n",
    "image = np.empty(2, dtype=object)\n",
    "label = np.empty(2, dtype=object)\n",
    "idx = 1\n",
    "for i in range(np.shape(image_paths1)[1]):\n",
    "    image_path = image_paths1[idx][i]\n",
    "    label[i] = labels1[idx][i]\n",
    "    image[i] = Image.open(image_path)\n",
    "# convert image to ndarray\n",
    "image = np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of repited index in the metadata data frame:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9120, 17)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Alireza\\Documents\\Git\\Cosine-Project\")\n",
    "from inSilico_experiments.utils.func_lib import *\n",
    "from inSilico_experiments.utils.pothook_analysis_lib import *\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# First let read all metadata data frames and concatenate them into one with one extera column for the image root path and one for the image folder\n",
    "# read other batch of metadata files\n",
    "data_root_batch = r\"N:\\PonceLab\\Users\\Alireza\\insilico_experiments\\Alexnet_remonstration_across_different_layer_062023\\raw_data\\cross_layer_recording_o2_062823\\meta_data_files\"\n",
    "# read all metadata files in the formt of \"*_most_*.h5\" folder and concatenate them into one\n",
    "# the metadata files are saved in h5 format with the key \"expriment_meta_data_df\"\n",
    "meta_data_df_batch = pd.DataFrame()\n",
    "for file in os.listdir(data_root_batch):\n",
    "    # we want to read all data with that have most in their name\n",
    "    if file.endswith(\".h5\") and \"random\" in file:\n",
    "        #print(f\"reading file {file}\")\n",
    "        metadata_df = pd.read_hdf((data_root_batch+ \"\\\\\"+ file), key=\"expriment_meta_data_df\")\n",
    "        # each file is saved with the name of the data folder with a suffix of a random number at the end of the name taht\n",
    "        # splited with \"_\" so we want to extract the data folder name from the file name and add it as a column to the metadata data frame\n",
    "        folder_name = file.split(\"_\")[1]\n",
    "        for si in file.split(\"_\")[2:-1]:\n",
    "            folder_name = folder_name +(\"_\"+si)\n",
    "\n",
    "        metadata_df[\"data_root\"] = os.path.join(r\"N:\\PonceLab\\Users\\Alireza\\insilico_experiments\\Alexnet_remonstration_across_different_layer_062023\\raw_data\\O2_cluster_exp_062123\",\n",
    "                                                folder_name)\n",
    "        print(\"number of repited index in the metadata data frame: \", metadata_df.index.duplicated().sum())\n",
    "        meta_data_df_batch = pd.concat([meta_data_df_batch, metadata_df], axis=0)\n",
    "        break\n",
    "\n",
    "metadata_df = meta_data_df_batch\n",
    "metadata_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read file with this format meta_data_alexnet_most_fc6_conv53_2_MSE_[digitpatern].h5 from data_root_batch\n",
    "pattern = \"meta_data_alexnet_most_fc6_conv53_2_MSE_*.h5\"\n",
    "matching_files = glob.glob(os.path.join(data_root_batch, pattern))\n",
    "matching_files[0].split(os.sep)[-1].split(\"_\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of repited index in the metadata data frame:  0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Alireza\\Documents\\Git\\Cosine-Project\")\n",
    "from inSilico_experiments.utils.func_lib import *\n",
    "from inSilico_experiments.utils.pothook_analysis_lib import *\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# First let read all metadata data frames and concatenate them into one with one extera column for the image root path and one for the image folder\n",
    "# read other batch of metadata files\n",
    "data_root_batch = r\"N:\\PonceLab\\Users\\Alireza\\insilico_experiments\\Alexnet_remonstration_across_different_layer_062023\\raw_data\\O2_cluster_exp_062123\\meta_data_files\"\n",
    "# read all metadata files in the formt of \"*_most_*.h5\" folder and concatenate them into one\n",
    "# the metadata files are saved in h5 format with the key \"expriment_meta_data_df\"\n",
    "meta_data_df_batch = pd.DataFrame()\n",
    "for file in os.listdir(data_root_batch):\n",
    "    # we want to read all data with that have most in their name\n",
    "    if file.endswith(\".h5\") and \"random\" in file:\n",
    "        #print(f\"reading file {file}\")\n",
    "        metadata_df = pd.read_hdf((data_root_batch+ \"\\\\\"+ file), key=\"expriment_meta_data_df\")\n",
    "        # each file is saved with the name of the data folder with a suffix of a random number at the end of the name taht\n",
    "        # splited with \"_\" so we want to extract the data folder name from the file name and add it as a column to the metadata data frame\n",
    "        folder_name = file.split(\"_\")[1]\n",
    "        for si in file.split(\"_\")[2:-1]:\n",
    "            folder_name = folder_name +(\"_\"+si)\n",
    "\n",
    "        metadata_df[\"data_root\"] = os.path.join(r\"N:\\PonceLab\\Users\\Alireza\\insilico_experiments\\Alexnet_remonstration_across_different_layer_062023\\raw_data\\O2_cluster_exp_062123\",\n",
    "                                                folder_name)\n",
    "        print(\"number of repited index in the metadata data frame: \", metadata_df.index.duplicated().sum())\n",
    "        meta_data_df_batch = pd.concat([meta_data_df_batch, metadata_df], axis=0)\n",
    "        break\n",
    "\n",
    "metadata_df = meta_data_df_batch\n",
    "\n",
    "RF_path = r\"N:\\PonceLab\\Users\\Alireza\\insilico_experiments\\Alexnet_remonstration_across_different_layer_062023\\post_processed\\rf_filters\"\n",
    "init_img_path = r\"N:\\PonceLab\\Users\\Alireza\\insilico_experiments\\Alexnet_remonstration_across_different_layer_062023\\post_processed\\init_img\\init_img.jpg\"\n",
    "init_img = Image.open(init_img_path)\n",
    "init_img = ToTensor()(init_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# add a new column to the dataframe to store the l2 distance between the generated image and the target image\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m metadata_df[\u001b[39m\"\u001b[39m\u001b[39msim_index\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnan\n\u001b[0;32m      3\u001b[0m row_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39m# loop over all rows of the meta data dataframe\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# add a new column to the dataframe to store the l2 distance between the generated image and the target image\n",
    "metadata_df[\"sim_index\"] = np.nan\n",
    "row_count = 0\n",
    "# loop over all rows of the meta data dataframe\n",
    "for i, row in metadata_df.iterrows():\n",
    "    if row[\"output_type\"] == \"best_gen_imgs_RF_masked\":\n",
    "        # load the image as tensor\n",
    "        img_path = glob.glob(os.path.join(row[\"data_root\"], f\"{i}.jpg\"))[0]\n",
    "        img = Image.open(img_path)\n",
    "        gen_image_tensor = ToTensor()(img)\n",
    "        # find target image id and load the image as tensor\n",
    "        i_target = find_target_image_id(metadata_df, row)\n",
    "        # load the target image as tensor\n",
    "        img_path = glob.glob(os.path.join(row[\"data_root\"], f\"{i_target}.jpg\"))[0]\n",
    "        img = Image.open(img_path)\n",
    "        target_image_tensor = ToTensor()(img)    \n",
    "\n",
    "        if row[\"layer_short\"] == \"conv5432\":\n",
    "            layer_short = \"conv5\"\n",
    "            pop_size = row[\"pop_size\"]*4\n",
    "        elif row[\"layer_short\"] == \"conv53\":\n",
    "            layer_short = \"conv5\"\n",
    "            pop_size = row[\"pop_size\"]*2\n",
    "        else:\n",
    "            layer_short = row[\"layer_short\"]\n",
    "            pop_size = row[\"pop_size\"]\n",
    "        \n",
    "        RF_map = np.load(os.path.join(RF_path, f\"{layer_short}_{pop_size}.npz\"))\n",
    "\n",
    "        RF_filter = RF_map[\"fitmap\"] > RF_map[\"fitmap\"][int(RF_map[\"xo\"]+(1.5*RF_map[\"sigma_x\"])), int(RF_map[\"yo\"]+(1.5*RF_map[\"sigma_y\"]))]\n",
    "\n",
    "        # calculate the l2 distance between the generated image and the target image and store it in the dataframe\n",
    "        try:\n",
    "            metadata_df.loc[i, \"sim_index\"] = \\\n",
    "                  normalized_pixel_similarity(target_image_tensor, gen_image_tensor, init_img, RFfilter=np.array(RF_filter))\n",
    "        except:\n",
    "            print(f\"error in calculating l2 distance for raw: {i}\")\n",
    "        #get_similarty_score_by_CNN(gen_image_tensor, target_image_tensor, scorer_dict, metadata_df, i)\n",
    "        row_count = row_count + 1\n",
    "        if row_count % 5000 == 0:\n",
    "            print(f\"processed {row_count} rows\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inSilico_experiments.utils.CosineDataset import SimpleDataset\n",
    "from torch.utils.data import DataLoader\n",
    "transform = Compose([\n",
    "    ToTensor()\n",
    "])\n",
    "re_image_type = \"best_gen_imgs_RF_masked\"\n",
    "target_image_type = \"target_img_RF_masked\"\n",
    "all_rec_labels = metadata_df[metadata_df[\"output_type\"] == re_image_type].index.values\n",
    "# let find the target images labels corresponding to the reconsructed images labels and save them in a np array\n",
    "all_target_labels = np.empty(len(all_rec_labels), dtype=object)\n",
    "for i, row_label in enumerate(all_rec_labels):\n",
    "    i_target = find_target_image_id(metadata_df, metadata_df.loc[row_label], targettype=target_image_type)\n",
    "    all_target_labels[i] = i_target\n",
    "# concatenate the two arrays to make a 2D array\n",
    "label_list = np.stack((all_rec_labels, all_target_labels), axis=1)\n",
    "path_list = [os.path.join(metadata_df.loc[row_label][\"data_root\"], f\"{i}.jpg\") for i in all_rec_labels]\n",
    "path_list = np.stack(([os.path.join(metadata_df.loc[row_label][\"data_root\"], f\"{i}.jpg\") for i in all_rec_labels],\n",
    "                            [os.path.join(metadata_df.loc[row_label][\"data_root\"], f\"{i}.jpg\") for i in all_target_labels]) , axis=1)\n",
    "\n",
    "dataset = SimpleDataset(path_list, label_list, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=5, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\Alireza\\miniconda3\\envs\\cosine-project\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\Alireza\\miniconda3\\envs\\cosine-project\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"c:\\Users\\Alireza\\miniconda3\\envs\\cosine-project\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 264, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"c:\\Users\\Alireza\\miniconda3\\envs\\cosine-project\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"c:\\Users\\Alireza\\miniconda3\\envs\\cosine-project\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 169, in collate_numpy_array_fn\n    raise TypeError(default_collate_err_msg_format.format(elem.dtype))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m labels_batch \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m      2\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alireza\\miniconda3\\envs\\cosine-project\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Alireza\\miniconda3\\envs\\cosine-project\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1345\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1346\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[1;32mc:\\Users\\Alireza\\miniconda3\\envs\\cosine-project\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[0;32m   1371\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1372\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[0;32m   1373\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Alireza\\miniconda3\\envs\\cosine-project\\lib\\site-packages\\torch\\_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[1;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\Alireza\\miniconda3\\envs\\cosine-project\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\Alireza\\miniconda3\\envs\\cosine-project\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"c:\\Users\\Alireza\\miniconda3\\envs\\cosine-project\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 264, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"c:\\Users\\Alireza\\miniconda3\\envs\\cosine-project\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"c:\\Users\\Alireza\\miniconda3\\envs\\cosine-project\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 169, in collate_numpy_array_fn\n    raise TypeError(default_collate_err_msg_format.format(elem.dtype))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object\n"
     ]
    }
   ],
   "source": [
    "for labels_batch in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from inSilico_experiments.utils.CosineDataset import SimpleDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "        assert len(self.image_paths) == len(self.labels), \"Number of images and labels must be equal\"\n",
    "        assert len(np.shape(self.labels)) == len(np.shape(self.image_paths)), \"Dimensionality of labels must match dimensionality of image paths\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(np.shape(self.image_paths)) == 1:\n",
    "            image_path = self.image_paths[idx]\n",
    "            label = self.labels[idx]\n",
    "            # Load image using PIL\n",
    "            image = Image.open(image_path)\n",
    "            # Apply transformations, if specified\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "        elif len(np.shape(self.image_paths)) == 2:\n",
    "            image = np.empty(np.shape(self.image_paths)[1], dtype=object)\n",
    "            label = np.empty(np.shape(self.image_paths)[1], dtype=object)\n",
    "            for i in range(np.shape(self.image_paths)[1]):\n",
    "                image_path = self.image_paths[idx][i]\n",
    "                label[i] = self.labels[idx][i]\n",
    "               \n",
    "        else:\n",
    "            raise ValueError(\"Dimensionality of image paths must be 1 or 2\")    \n",
    "\n",
    "        return label\n",
    "\n",
    "transform = Compose([\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "dataset = SimpleDataset(path_list, label_list, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=5, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for labels_batch in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([4, 7]), tensor([5, 8]), tensor([6, 9])]\n",
      "[tensor([1]), tensor([2]), tensor([3])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        return (item)\n",
    "\n",
    "# Example usage\n",
    "data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "custom_dataset = CustomDataset(data)\n",
    "\n",
    "# Create a DataLoader from the custom dataset\n",
    "# Set batch_size to the desired batch size (e.g., 2)\n",
    "# Set shuffle to True if you want to shuffle the data during training\n",
    "data_loader = DataLoader(custom_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Accessing batches using DataLoader\n",
    "for batch in data_loader:\n",
    "    print(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosine-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
